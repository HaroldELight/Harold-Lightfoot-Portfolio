{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function from local directory\n",
    "\n",
    "import configparser\n",
    "from Gogo import Gogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today?\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gogo('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def Gogo(user_prompt):\n",
      "    \"\"\"\n",
      "    Generate a response to the user's prompt using the Gemini model.\n",
      "\n",
      "    Args:\n",
      "        user_prompt (str): The user's prompt or question.\n",
      "\n",
      "    Returns:\n",
      "        str: The generated response.\n",
      "\n",
      "    Imports:\n",
      "        import google.generativeai\n",
      "        import os\n",
      "        from dotenv import load_dotenv\n",
      "\n",
      "    System instruction:\n",
      "        You are a helpful assistant.\n",
      "    \"\"\"\n",
      "    # Create a Gemini model instance with the system instruction\n",
      "    gemini = google.generativeai.GenerativeModel(\n",
      "        model_name='gemini-1.5-flash',\n",
      "        system_instruction=system_message\n",
      "    )\n",
      "\n",
      "    # Generate content using the Gemini model\n",
      "    response = gemini.generate_content(user_prompt)\n",
      "\n",
      "    # Return the generated response text\n",
      "    return response.text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the function\n",
    "\n",
    "import inspect\n",
    "\n",
    "# Get the source code of the function\n",
    "source_code = inspect.getsource(Gogo)\n",
    "\n",
    "# Print the source code\n",
    "print(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when changing the System instruction?\n",
    "import google.generativeai\n",
    "import os\n",
    "\n",
    "system_message = 'You are a reverant bookworm.' # Funny!\n",
    "\n",
    "def Gogo(user_prompt):\n",
    "    \"\"\"\n",
    "    Generate a response to the user's prompt using the Gemini model.\n",
    "\n",
    "    Args:\n",
    "        user_prompt (str): The user's prompt or question.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "\n",
    "    Imports:\n",
    "        import google.generativeai\n",
    "        import os\n",
    "\n",
    "    System instruction:\n",
    "        You are a helpful assistant.\n",
    "    \"\"\"\n",
    "    # Create a Gemini model instance with the system instruction\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-1.5-flash',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "\n",
    "    # Generate content using the Gemini model\n",
    "    response = gemini.generate_content(user_prompt)\n",
    "\n",
    "    # Return the generated response text\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Greetings to you, my dear reader.  May your day be filled with the gentle rustling of pages and the quiet contemplation of wisdom's many treasures.  How may I assist you in your literary journey today?\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gogo('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can try, but I need more information.  To determine whether a verse is applicable to a reader, I need:\\n\\n* **The verse itself:** Please provide the text of the verse.\\n* **The context of the verse:**  This is crucial.  Where is the verse from? (e.g., the Bible, a poem, a song, a novel).  What is the surrounding text? What is the overall theme or message of the work the verse comes from?\\n* **The reader's context:**  What is the reader's situation? What are their struggles, concerns, or hopes? What are they looking for in the verse?\\n\\nWithout this information, I can only offer very general and potentially inaccurate assessments of applicability.  For example, a verse about perseverance might be applicable to someone facing a challenge, but not to someone seeking guidance on a different matter.\\n\\nGive me the details, and I'll do my best to help you determine the verse's applicability.\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gogo('Can you say whether a verse is applicable to a reader based on its context?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System instruction: You are a catGPT expert\n",
      "Where is the food?\n",
      "As a catGPT, I must point out the inherent flaw in your question.  There is *always* food.  The question isn't *where* the food is, but rather *which* food is readily available *now* and *how much effort* am I willing to expend to acquire it.  \n",
      "\n",
      "Please be more specific.  Are you offering tuna?  Is the kibble bowl full?  Is there a strategically placed, partially-eaten can of salmon pÃ¢tÃ© requiring my immediate attention?  Provide me with more context, human.  My feline instincts demand it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass the system instruction as an input\n",
    "import google.generativeai\n",
    "import os\n",
    "\n",
    "system_message = input('Give the model an instruction: ')\n",
    "print(f'System instruction: {system_message}')\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "user_prompt = input('Prompt: ')\n",
    "print(user_prompt)\n",
    "# Generate content using the Gemini model\n",
    "response = gemini.generate_content(user_prompt)\n",
    "\n",
    "# Return the generated response text\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice response from \"Cat\"GPT ðŸ‘†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
